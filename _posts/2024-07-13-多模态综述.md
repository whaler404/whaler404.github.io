---
layout: post
title:  多模态论文综述
# subtitle: 
date:   2024-06-24
author: whaler404
header-img: img/post-bg-playphone.jpg
categories: post ml
catalog: true
tags:
    - DL
    - Thesis
---

# 视觉理解

Visual Understanding

## 监督学习

监督学习 Supervised Learning

- [如何理解机器学习中的嵌入 (Embedding)](https://www.cnblogs.com/ghj1976/p/ru-he-li-jie-ji-qi-xue-xi-zhong-de-qian-ru-embeddi.html)
- [CNN中 patch 是什么？](https://blog.csdn.net/wills798/article/details/97974617)
- [大模型微调方法：冻结方法 Freeze、P-Tuning 系列、LoRA、QLoRA](https://blog.csdn.net/qq_41739364/article/details/134971066)
- [浅析深度学习中的mask操作](https://blog.csdn.net/guofei_fly/article/details/104505795)

## 对比图像-语言预训练

对比图像-语言预训练 Contrastive Image-Langeage Pre-training

- [多模态领域的开源图文数据集(持续更新中20230511)](https://www.cnblogs.com/chentiao/p/17381616.html)
- [nlp中常说的对齐-Alignment](https://www.cnblogs.com/chentiao/p/17388152.html)
- [零样本学习(Zero-Shot Learning)简介与分类](https://zhuanlan.zhihu.com/p/436720853)
- [对比学习正负例在干什么？](https://blog.csdn.net/m0_37525990/article/details/115092677)

### CLIP 基础

CLIP 基础（Basic of CLIP）

### CLIP 变体

CLIP 变体（CLIP Variants）

## 仅图像的自监督学习

仅图像的自监督学习（Image-only Self-supervised Learning）

### 对比学习和非对比学习

对比学习和非对比学习（Contrastive and Non-contrastive Learning）

### 掩码图像建模

掩码图像建模（Masked Image Modeling）

- [用transformer做视觉，具体是怎么把图片转成token的？](https://www.zhihu.com/question/488561011)
- [nlp 特殊标记符](https://www.cnblogs.com/pass-ion/p/17662361.html)
- [一文讲解方向梯度直方图（hog）](https://zhuanlan.zhihu.com/p/85829145)

## 不同模式之间的协同作用

不同模式之间的协同作用（Synery among Different Models）

## 多模态融合

多模态融合（Multimodal Fusion）

## 区域级和像素级的预训练

区域级和像素级的预训练（Region-level and Pixel-level Pre-training）

### 从多模态融合到多模态 LLM

From Multimodal Fusion to Multimodal LLM

对于 CLIP 等双编码器，图像和文本分别编码，通过二者的特征向量的点积处理进行模态交互，缺乏深度的多模态融合。在零样本图像分类、图像文本检索任务有效，在图生文、视觉问答任务效果表现不佳。

所以使用预训练的融合编码器（fusion encoder），增加额外的 transformer 层来模拟图像和文本表征的深度融合。

- OD-based models：早期使用
    - 预训练的对象检测器（object detector）提取视觉特征，比如 R-CNN 加位置编码
    - 文本令牌嵌入（token embedding）提取文本特征
    - ViLBERT 等使用共同注意力（co-attention）进行多模态融合
    - UNIBERT 使用图像特征作为文本输入的软提示（soft prompt），包括掩码语言建模（MLM）、掩码区域建模（MRM）、单词区域对齐（WRA）+ 图像文本匹配（ITM），然后传入多模态 transformer
- End-to-end models：已成为主流
    - 采用 ViT 作为图像编码器，比如 Coca
    - Coca 使用对比损失进行对齐，字幕损失（captioning loss）多模态表征
- Trend to multimodal LLM：
    - SimVLM 使用 PrefixLM 损失预训练，从大规模预训练转为使用 LLM 进行指令调整（instruction tuning）

### 区域级的预训练

CLIP 通过预训练学习图像的全局表示，但是在细粒度图像理解任务中，比如目标检测，效果不佳。目标检测任务包括定位和识别，识别可以转化为 CLIP 的图像检索，实现通用的开放集目标检测，该主题在 `4.2` 节有更详细的展开。

### 像素级的预训练

SAM（segment anything model）模型是最近语义分割领域的视觉基础模型，使用了像素级（pixel-level）的预训练。SAM 可以适应许多分割任务，包括边缘检测、对象提议生成（object proposal generation）、实例分割、开放词汇分割（open-vocabulary segmentation）

Segment Anything 项目通过引入三个相关联的组件来构建用于分割的视觉基础模型：
- 可提示分割任务：能够在给定任何分割提示下，返回有效的分割掩码
- 分割模型：SAM 由三个组件组成
    - 图像编码器，使用预训练的 ViT 
    - 提示编码器：使用 CLIP 文本编码器，处理点、框（box）、自由形式的文本等稀疏（sparse）数据类型；使用卷积算子，处理掩码等密集（dense）数据类型。
    - 轻量级掩码解码器：基于 transformer
- 数据引擎：执行模型内循环（model-in-the-loop）的数据集注释（annotation），用于获取预训练的大规模数据（large-scale）



# 视觉生成

视觉生成（Visual Generation）

## 文本到图像的生成

文本到图像的生成（Text-to-image Generation）

## 空间可控生成

空间可控生成（Spatial Controllable Generation）

## 基于文本的编辑

基于文本的编辑（Text-based Editing）

## 文本提示

文本提示（Text Prompts Following）

## 概念定制

概念定制（Concept Customization）

# 统一视觉模型

统一视觉模型（Unified Vision Models）

## 从封闭集模型到开放集模型

从封闭集模型到开放集模型（From Close-set to Open-set Models）

## 从特定任务到通用模型

从特定任务到通用模型（From Task-specific to Generic Models）

## 从静态模型到可提示模型

从静态模型到可提示模型（From Static to Promptable Models）

# 使用 LLM 进行大型多模态模型训练

使用 LLM 进行大型多模态模型训练（Large Multi-Modal Models training with LLM）

## 图像到文本的生成

图像到文本的生成（Image-to-text Generation）

# 多模式代理：使用 LLM 链接工具

多模式代理：使用 LLM 链接工具（Multimodal Agents: Chaining Tools with LLM）

## 多模式代理

多模式代理（Multimodal Agent）

# 相关资料

- [用transformer做视觉，具体是怎么把图片转成token的？](https://www.zhihu.com/question/488561011)
- [如何估计大模型的参数量和占用显存](https://zhuanlan.zhihu.com/p/643950399)
- [机器学习时代的哈希算法，将如何更高效地索引数据](https://zhuanlan.zhihu.com/p/36491759)
- [⾼维特征的哈希技巧](https://zhuanlan.zhihu.com/p/161058660)
- [具有多分辨率的哈希编码](https://blog.csdn.net/MengYa_Dream/article/details/123795451)

# 论文解读

- [【论文解读】多模态大模型综述](https://www.cnblogs.com/intsig/p/18067895)
- [对比学习（Contrastive Learning）综述](https://zhuanlan.zhihu.com/p/346686467)
- [【CLIP系列Paper解读】CLIP: Learning Transferable Visual Models From Natural Language Supervision](https://zhuanlan.zhihu.com/p/486857682)



1. nerf 和高斯泼溅
五维度映射到实数值，相机的内参和外参，隐式；点云，显式表达；3d高斯结合显式和隐式，前者空间换时间，后者实值表达。nerf数据集，多张图合成3d，学习相机内参外参构建映射？有背景无背景数据集，
评价指标，PSNR峰值信噪比对比噪声和干净样本，ssim结构相似性对比图片相似度，lpips学习感知图像块相似度深度特征衡量相似度。隐式+静态，合成新视角。训练用了残差结构+位置编码，f（x,y,z,实现夹角）->f（颜色，密度权重），会出现对特定视角的过拟合（2d好但3d烂），所以视角最后加入训练；位置编码用于增强细节。体渲染：光吸收+光发射，构建3d，然后拍照计算误差，外参有了，算内参。colmap，三维高斯模型

2. 融合时序，流式处理，tmm，多分辨率哈希编码

