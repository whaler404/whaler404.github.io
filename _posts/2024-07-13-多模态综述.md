---
layout: post
title:  多模态论文综述
# subtitle: 
date:   2024-06-24
author: whaler404
header-img: img/post-bg-playphone.jpg
categories: post ml
catalog: true
tags:
    - DL
    - Thesis
---

# 视觉理解

Visual Understanding

## 2.2 概述

图像骨干预训练 image backbone pretraining
- 标签监督 label supervision
- 语言监督 language supervision
- 仅图像自监督 image-only self-supervised Learning
    - 对比学习 contrastive learning
    - 非对比学习 non-contrastive learning
    - 掩码图像建模 masked image modeling

进一步预训练多模态融合和细粒度的图像理解 Further pre-training for multimodal fusion andfine-grained image understanding
- 多模态融合 multimodal fusion
- 区域级预训练 region-level pre-training
- 像素级预训练 pixel-level pre-training


## 2.2 监督学习

监督学习 Supervised Learning

[如何理解机器学习中的嵌入 (Embedding)](https://www.cnblogs.com/ghj1976/p/ru-he-li-jie-ji-qi-xue-xi-zhong-de-qian-ru-embeddi.html)<br>
[CNN中 patch 是什么？](https://blog.csdn.net/wills798/article/details/97974617)<br>
[大模型微调方法：冻结方法 Freeze、P-Tuning 系列、LoRA、QLoRA](https://blog.csdn.net/qq_41739364/article/details/134971066)<br>
[浅析深度学习中的mask操作](https://blog.csdn.net/guofei_fly/article/details/104505795)<br>

## 2.3 对比图像-语言预训练

对比图像-语言预训练 Contrastive Image-Langeage Pre-training

[多模态领域的开源图文数据集(持续更新中20230511)](https://www.cnblogs.com/chentiao/p/17381616.html)<br>
[nlp中常说的对齐-Alignment](https://www.cnblogs.com/chentiao/p/17388152.html)<br>
[零样本学习(Zero-Shot Learning)简介与分类](https://zhuanlan.zhihu.com/p/436720853)<br>
[对比学习正负例在干什么？](https://blog.csdn.net/m0_37525990/article/details/115092677)

### 2.3.1 CLIP 基础

CLIP 基础（Basic of CLIP）
- 训练 training：模型训练需要沿三个维度进行缩放：批量大小、数据大小和模型大小；模型大小上从 300M (Large) 到 1B (giant) 不等
- 零样本预测 zero-shot prediction：CLIP 将零样本图像分类任务变为检索任务，考虑标签背后的语义，从而实现零样本图像分类

### 2.3.2 CLIP 变体

CLIP 变体（CLIP Variants）
1. 增大数据集规模 data scaling up
2. 模型设计和训练方法 model design and training methods
    - image tower：随机屏蔽高掩码率的patch，只编码可见patch
    - text tower：丰富文本描述，比如 ChatGPT
    - interpret ability：图像和文本向量映射到高维系数空间，提高可解释性
    - more modalities：引入更多模态
3. 目标函数 objective function
    - 细粒度监督 fine-grained supervision：学习word-patch对齐，而不是点积计算text-image相似度
    - contrastive captioner：对比损失和生成标题损失
    - captioning loss only：仅生成标题损失

## 2.4 仅图像的自监督学习

仅图像的自监督学习（Image-only Self-supervised Learning）

### 2.4.1 对比学习和非对比学习

对比学习和非对比学习（Contrastive and Non-contrastive Learning）

对比学习 contrastive learning：比如 SimCLR、MoCo

SimCLR
1. 给定一个图像，应用两个独立的数据增强（data augmentation）
2. 基础编码器后跟着一个项目头（project head），训练时使用对比损失（contrastive loss）最大化一致性（agreement）
3. 下游任务（downstream task）时去掉项目头
> 依赖负样本，需要大内存

非对比学习 non-contrastive learning：SimSiam、DINO

SimSiam：
1. 给定一个图像，生成两个增强视图，使用相同的编码器网格处理
2. 预测 MLP 运用于一个视图，另一个视图停止梯度操作
3. 最大化两个视图间的相似性
> 非对称结构，不依赖负对和动量编码器

[动量编码器- Laplace蒜子](https://www.cnblogs.com/RedNoseBo/p/17839698.html)

DINO：
1. 给定一个图像，生成两个增强视图，分别输入到学生、教师网络中
2. 两个网络共享相同的架构，但参数不同，教师网络输出为 batch 的均值
3. 两个网络输出的特征向量使用带温度的 softmax 归一化（temperature softmax）
4. 停止梯度向教师网络传播，仅通过学生网络传播，使用学生参数的指数移动平均（exponential moving average）更新教师参数

[深度学习高温蒸馏：Softmax With Temperature](https://zhuanlan.zhihu.com/p/504323465)


### 2.4.2 掩码图像建模

掩码图像建模（Masked Image Modeling）：BERT 使用掩码语言建模进行预训练，启发了 BEiT 使用掩码图像建模（MIM）进行预训练
1. 图像标记器 image tokenizer：图像转换为离散的视觉标记，比如 VQ-VAE、ViT-VQGAN
2. 掩码后预测 mask-then-predict：模型输入掩码图像，预测掩码位置的内容
> iBOT 将其描述为 image tokenizer （教师）和 BEiT 编码器（学生）之间的知识蒸馏（knowledge distillation）

MIM 的统一视图：教师模型、归一化层、学生模型、MIM 头、适当的损失函数。模型间的显著差别在于重建的目标（reconstruction target），可以是像素、离散的图像token、预训练模型输出特征、动量更新教师模型的输出。MIM 重建目标可以分为两大类：
- 低级像素/特征重建 low-level pixel/feature：MAE、SimMIM 等使用原始的或归一化的像素值作为重建目标，MaskFeat 引入定向梯度直方图（HOG）特征作为目标，Ge2-AE 使用2D离散傅里叶变换得到的像素值和频率信息作为目标。
- 高级特征重建 high-level feature：BEiT、CAE 使用学习的图像 token 作为重建目标，MaskFeat 直接将对从 DINO、DeiT 等模型学习到的特征作为目标，受此启发，EVA、MVP 使用 CLIP 的图像特征，其他方法提出了从动量更新的教师模型构建回归特征。

用于视频预训练的 MIM（MIM for vedio pre-training）：将 MIM 扩展到视频预训练的工作，比如 BEVT、MAE 作为时空学习器、VedioMAE 等。

缺乏全局图像表示学习（Lack of learning global image representation）：普通 MIM 对预训练和参数初始化有效，但是没有学习全局的图像表示。iBOT 提出使用 DINO 的自蒸馏损失（self-distillation loss）来增强全局图像表示学习，由此扩展出 DINOv2

MIM 的扩展属性（Scaling properties of MIM）：MIM 可以认为是有效的正则化手段，帮助下游任务初始化十亿规模的 ViT

[用transformer做视觉，具体是怎么把图片转成token的？](https://www.zhihu.com/question/488561011)<br>
[nlp 特殊标记符](https://www.cnblogs.com/pass-ion/p/17662361.html)<br>
[一文讲解方向梯度直方图（hog）](https://zhuanlan.zhihu.com/p/85829145)

## 2.5 不同模式之间的协同作用

不同模式之间的协同作用（Synery among Different Models）

结合标签监督的 CLIP（combine CLIP with label supervision）:
- UniCL：在联合图像-文本-标签空间中，使用图像-标签和图像-文本数据进行统一对比学习
- LiT：教授文本模型从预训练好的图像模型中读到好的表示，用于新任务的完成
- MOI：从10亿个noisy的实体注释图像中学习图像表示，使用图像分类和对比损失进行训练，

结合仅图像（非）对比学习的 CLIP（combine CLIP with image-only (non-)contrastive learning）：
- SLIP 结合 SimCLR 和 CLIP，在零样本迁移（zero-shot transfer）和线性探测（linear probe）上，SLIP 比 CLIP 更好；DeCLIP 在各个模态上挖掘自监督信号来增强 CLIP 训练。
- xCLIP 从仅图像的非对比学习中引入锐度（sharpness）和平滑（smoothness）正则项来实现 CLIP 非对比。

[【Linear Probing | 线性探测】深度学习 线性层](https://blog.csdn.net/LemonShy2019/article/details/125852323)

结合 MIM 的 CLIP（combine CLIP with MIM）：
- 浅交互（shallow interaction）：从 CLIP 中提取的特征作为 MIM 训练的目标，因为 CLIP 图像特征可能会捕获 MIM 训练中缺少的语义（semantics），而 MIM 预训练图像主干用于初始化 CLIP 训练。在这方面的工作中，MVP、EVA 直接回归 CLIP 的特征，而 BEiTv2 先将 CLIP 特征压缩成视觉 token，然后进行常规的 BEiT 训练。MIM 表示用于微调各种下游任务，而冻结的 CLIP 嵌入可以实现零样本图像分类和其他应用。
- 更深的交互（deeper interaction）：没什么用

## 2.6 多模态融合、区域级和像素级的预训练

多模态融合（Multimodal Fusion）、区域级和像素级的预训练（Region-level and Pixel-level Pre-training）

### 2.6.1 从多模态融合到多模态 LLM
Fom Multimodal Fusion to Multimodal LLM 

对于 CLIP 等双编码器，图像和文本分别编码，通过二者的**特征向量的点积处理进行模态交互，缺乏深度的多模态融合**。在零样本图像分类、图像文本检索任务有效，在图生文、视觉问答任务效果表现不佳。

所以使用预训练的融合编码器（fusion encoder），增加额外的 transformer 层来模拟图像和文本表征的深度融合。

- 基于目标检测的模型（OD-based models）：早期使用。UNITER 通过现成的对象检测器提取图像特征，并将图像特征视为文本输入的软提示（soft prompt），包括掩码语言建模（MLM）、掩码区域建模（MRM）、单词区域对齐（WRA）+ 图像文本匹配（ITM），然后传入多模态 transformer。其中对象检测器使用 R-CNN 加位置编码，文本令牌嵌入（token embedding）提取文本特征。
- 端到端模型（End-to-end models）：已成为主流。Coca 使用对比损失进行对齐，字幕损失（captioning loss）多模态表征。其中图像编码器采用 ViT，使用共同注意力（co-attention）进行多模态融合。
- 多模态LLM的趋势（Trend to multimodal LLM）：SimVLM 使用简单的 PrefixLM 损失预训练，从大规模预训练转为使用 LLM 进行指令调整（instruction tuning），在此之后多模态语言模型开始流行起来。

### 2.6.2 区域级的预训练

CLIP 通过预训练学习图像的全局表示，但是在细粒度图像理解任务中，比如目标检测，效果不佳。目标检测任务包括定位和识别，识别可以转化为 CLIP 的图像检索，实现通用的开放集目标检测，该主题在 `4.2` 节有更详细的展开。

### 2.6.3 像素级的预训练

SAM（segment anything model）模型是最近语义分割领域的视觉基础模型，使用了像素级（pixel-level）的预训练。SAM 可以适应许多分割任务，包括边缘检测、对象提议生成（object proposal generation）、实例分割、开放词汇分割（open-vocabulary segmentation）

Segment Anything 项目通过引入三个相关联的组件来构建用于分割的视觉基础模型：
- **可提示分割任务**：能够在给定任何分割提示下，返回有效的分割掩码
- **分割模型**：SAM 由三个组件组成
    - **图像编码器**，使用预训练的 ViT 
    - **提示编码器**：使用 CLIP 文本编码器，处理点、框（box）、自由形式的文本等稀疏（sparse）数据类型；使用卷积算子，处理掩码等密集（dense）数据类型。
    - **轻量级掩码解码器**：基于 transformer
- **数据引擎**：执行模型内循环（model-in-the-loop）的数据集注释（annotation），用于获取预训练的大规模数据（large-scale）

除了 SAM 外的其他工作：OneFormer 提出通用的图像分割框架，SegGPT 提出了通用的上下文学习框架，SEEM 扩展了支持的提示类型，包括点、框、涂鸦、掩码、文本、引用区域。

# 3 视觉生成

视觉生成（Visual Generation），T2I 基础模型：DALL-E 、Stable Diffusion、Midjourney、MUSE 等。

## 3.1 概述

### 3.1.1 视觉生成中的人类对齐

human-alignment in vision generation：文生图背景下的 AI 对齐是 T2I 模型的研究领域，主要有四个常见的研究问题：
- **空间可控的文生图**（spatial-controllable T2I generation）：文本是人机交互的强大媒介，但是单纯的文本不足以提供准确的空间参考，空间可控的 T2I 模型将文本输入和其他条件结合获得更好的可控性。
- **基于文本的图像编辑**（text-based image editing）：图像编辑是获取人类预期（human-intended）视觉内容的另一个方法，包括局部和全局的修改
- **更好的后续文本提示**（better following text prompts）
- **视觉概念自定义**（visual concept customization）：视觉元素包含难以用单词描述的细节，已经有相关工作使用专门的 token嵌入和条件图像来生成图片，

### 3.1.2 文本到图像的生成

文本到图像的生成（Text-to-image Generation）：使用图像-文本对进行训练，文本作为输入条件，配对图像作为输出条件

- **生成对抗网络**（generative adversarial network，GAN）：**由两个组件组成，生成器（generator）和鉴别器（discriminator）**；生成器从随机噪声中创建合成图像，根据输入文本条件训练噪声，生成语义相关（semantically relevant）的图像；对抗过程中，鉴别器和生成器竞争，试图区分生成的图像和真实的图像，从而引导生成器提高图像创建能力。
- **变分自编码器**（variational autoencoder，VAE）：**概率模型，通过编码器和解码器模块来生成图像**；编码器将图像转换为潜在表示（latent representation），解码器将向量转换为图像。VAE 最小化原始图像和生成图像的 KL 散度（KL divergence）。矢量化 VAE（vector quantised VAE，VQVAE）利用适量量化离散的潜在空间提高了图像重建质量。
- **离散的图像 token 预测**（discret image token prediction）：**图像 tokenizer 和 detokenizer 的组合**，将连续的视觉信号转化为有限的离散 token，图像生成问题转化为离散 token 预测任务。一种策略是，使用自回归 transformer，从左上角开始顺序生成图像 token，以文本输入为提示逐行移动；另一种策略是，并行解码，加快预测过程。最后 detokenizer 解码得到预测的图像。
- **扩散模型**（diffusion model）：采用随机微分方程（stochastic differential equation）**将噪声演化为图像**，扩散模型在初始化时，使用完全随机的噪声，以输入的文本为条件，在去噪的过程中逐渐细化，每次迭代都会预测并删除噪声元素，图像连续演变。

SD 广泛使用的 T2I 模型：开源，基于扩散的生成模型，**基于交叉注意力（cross-attention-based）的图像文本融合机制**；主要包含*三个模块，**图像 VAE、去噪 U-Net、条件编码器**
- VAE：成对的编码器 $\mathcal{E}$ 和解码器 $\mathcal{D}$ ，编码RGB图像 $x\in R^{H\times W\times 3}$ 到潜在随机变量 $z\in R^{h\times w\times c}$ ，潜在变量 z 比图像 x 小48倍，通过在压缩的潜在紧凑空间（compressed compact latent space）中执行去噪处理，提高计算效率。
- Text Encoder：SD 是条件图像生成模型，使用条件编码器 $\tau(y)\in R^{N\times d_\tau}$ 
- Denoising U-Net：模块训练时，预测噪声 $\hat{\epsilon}(z_t,t)$ ，然后在潜在空间中减去噪声，逐步将随机初始噪声演化为有意义的潜在表示，使用 L2 比较实际噪声和目标噪声损失；模块推断时，从随机噪声开始对潜在表示 z 去噪。

降噪 U-Net 和传统 U-Net 相似，包括一系列跳跃连接的下采样/上采样块，每个块有一个交叉注意力层和2D卷积下采样/上采样层。交叉注意力层中，文本 $\tau$ 对图像 z 查询，交叉注意力图 $M=(hw\times d)\cdot(N\times d)^T=hw\times N$

[论文解读（GAN）《Generative Adversarial Networks》](https://www.cnblogs.com/BlairGrowing/p/15853567.html)<br>
[An In-Depth Guide to Denoising Diffusion Probabilistic Models DDPM – Theory to Implementation](https://learnopencv.com/denoising-diffusion-probabilistic-models/)<br>
[由浅入深了解Diffusion Model](https://zhuanlan.zhihu.com/p/525106459)<br>
[图像分割之U-Net](https://zhuanlan.zhihu.com/p/43927696)

## 3.2 空间可控生成

空间可控生成（Spatial Controllable Generation）的三个思路

**区域可控的 T2I 生成**（Region-controlled T2I generation）：
- **ReCo：扩展文本编码器的文本词汇**，位置标记和文本标记混合，使用空间修饰符表明后面的文本仅在指定的区域上，用以增强对文本标记的理解，然后对预训练的 T2I 模型微调。该方法提供了对象级的图像文本关联。
- **GLIGEN**：使用即插即用（plug-and-play）的方法，冻结原始 T2I 模型，使用文本语义和定位（grounding）信息，**训练额外的门控注意力层**（gated self-attention）学习新的联结（grounding）。支持多种空间信息（grounded control），包括边界框（bounding box）、关键点（keypoint）、图像提示（prompting）

**密集条件的 T2I 生成**（T2I generation with dense conditions）：
ControlNet 建立在 SD 的基础上，额外引入可训练 ControlNet 模块，支持多种条件（reference 垫图）。<br>每个条件分支有独自的模型副本，分支从预训练的 SD 的下采样模块初始化，后接零初始化（zero-initialized）的 1×1 卷积层作为门控连接器，然后将输出合并到基础 SD 的上采样模块中。<br>整个过程概括为：**克隆预训练模型，定义输入条件，训练可训练副本（trainable copy），合并输出**，既包含从预训练中学到的通用知识，也包含从微调中学到的特定知识。

**推理时的空间引导**（Inference-time spatial guidance）：类似于分类器指导，使用判别器损失（discriminator loss）指导扩散过程，判别器使用比如 Fast-RCNN 目标检测器。结果不如微调精细。

[多模态NLP研究中的grounding怎么翻译比较好？](https://www.zhihu.com/question/465618962)<br>
[FLASH：高效Transformer解析(3)---GAU(Gate Attention Unit)](https://zhuanlan.zhihu.com/p/487423771)<br>
[GiantPandaCV论文解读 Open-Set Grounded Text-to-Image Generation——GLIGEN](http://giantpandacv.com/academic/%E7%AE%97%E6%B3%95%E7%A7%91%E6%99%AE/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%20Open-Set%20Grounded%20Text-to-Image%20Generation/)<br>
[扩散模型大杀器 ControlNet 解析](http://giantpandacv.com/academic/%E7%AE%97%E6%B3%95%E7%A7%91%E6%99%AE/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E5%A4%A7%E6%9D%80%E5%99%A8%20ControlNet%20%E8%A7%A3%E6%9E%90/)<br>
[深度学习中的初始化方法(Initialization)](https://0809zheng.github.io/2020/03/05/initialization.html)

## 3.3 基于文本的编辑

基于文本的编辑（Text-based Editing）

[SDEdit论文笔记](https://juejin.cn/post/7158245434455457805)

## 文本提示

文本提示（Text Prompts Following）

## 概念定制

概念定制（Concept Customization）

# 统一视觉模型

统一视觉模型（Unified Vision Models）

## 从封闭集模型到开放集模型

从封闭集模型到开放集模型（From Close-set to Open-set Models）

## 从特定任务到通用模型

从特定任务到通用模型（From Task-specific to Generic Models）

## 从静态模型到可提示模型

从静态模型到可提示模型（From Static to Promptable Models）

# 使用 LLM 进行大型多模态模型训练

使用 LLM 进行大型多模态模型训练（Large Multi-Modal Models training with LLM）

## 图像到文本的生成

图像到文本的生成（Image-to-text Generation）

# 多模式代理：使用 LLM 链接工具

多模式代理：使用 LLM 链接工具（Multimodal Agents: Chaining Tools with LLM）

## 多模式代理

多模式代理（Multimodal Agent）

# 相关资料

- [用transformer做视觉，具体是怎么把图片转成token的？](https://www.zhihu.com/question/488561011)
- [如何估计大模型的参数量和占用显存](https://zhuanlan.zhihu.com/p/643950399)
- [机器学习时代的哈希算法，将如何更高效地索引数据](https://zhuanlan.zhihu.com/p/36491759)
- [⾼维特征的哈希技巧](https://zhuanlan.zhihu.com/p/161058660)
- [具有多分辨率的哈希编码](https://blog.csdn.net/MengYa_Dream/article/details/123795451)

# 论文解读

- [【论文解读】多模态大模型综述](https://www.cnblogs.com/intsig/p/18067895)
- [对比学习（Contrastive Learning）综述](https://zhuanlan.zhihu.com/p/346686467)
- [【CLIP系列Paper解读】CLIP: Learning Transferable Visual Models From Natural Language Supervision](https://zhuanlan.zhihu.com/p/486857682)



1. nerf 和高斯泼溅
五维度映射到实数值，相机的内参和外参，隐式；点云，显式表达；3d高斯结合显式和隐式，前者空间换时间，后者实值表达。nerf数据集，多张图合成3d，学习相机内参外参构建映射？有背景无背景数据集，
评价指标，PSNR峰值信噪比对比噪声和干净样本，ssim结构相似性对比图片相似度，lpips学习感知图像块相似度深度特征衡量相似度。隐式+静态，合成新视角。训练用了残差结构+位置编码，f（x,y,z,实现夹角）->f（颜色，密度权重），会出现对特定视角的过拟合（2d好但3d烂），所以视角最后加入训练；位置编码用于增强细节。体渲染：光吸收+光发射，构建3d，然后拍照计算误差，外参有了，算内参。colmap，三维高斯模型

2. 融合时序，流式处理，tmm，多分辨率哈希编码

