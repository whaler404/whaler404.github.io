---
layout: post
title:  机器学习——贝叶斯Bayes
# subtitle: 
date:   2024-05-26
author: whaler404
header-img: img/post-bg-playphone.jpg
categories: post ml
catalog: true
tags:
    - ML
---

# 贝叶斯决策论

贝叶斯决策论（bayesian decision theory）是在概率框架下实施决策的基本方法
- 在分类问题情况下，在所有相关概率都已知的理想情形下，贝叶斯决策考虑如何基于这些**概率和误判损失**来选择最优的类别标记

假设有N中可能的类别标记，即 $y=\{c_1,\dots,c_N\}$ ， $\lambda_ij$ 是将一个真实标记为 $c_j$ 的样本误分类为 $c_i$ 所产生的损失。基于后验概率 $P(c_i|x)$ 可获得将样本 $x$ 分类为 $c_i$ 所产生的期望损失（expected loss），即在样本上的“条件风险”（conditional risk）

$$
R(c_i|x)=\sum\limits_{i=1}^N\lambda_{ij}P(c_j|x)
$$

寻找一个判定准则 $h:X\mapsto Y$ 以最小化总体风险

$$
R(h)=E_x[R(h(x)|X)]
$$

显然，对每个样本 $x$ ，若规则 $h$ 能够最小化条件风险 $R(h(x)|x)$ ，则总体风险 $R(h)$ 也将被最小化

于是，得到贝叶斯判定准则（bayes decision rule）：为最小化总体风险，只需在每个样本上选择那个能使条件风险 $R(c|x)$ 最小的类别标记，即

$$
h^*(x)=\argmin\limits_{c\in y}R(c|x)
$$

- 此时，被称为贝叶斯最优分类器（bayes optimal classification），与之对应的总体风险 $R(h^*)$ 称为贝叶斯风险（bayes risk）
- $1-R(h^*)$ 反映了分类器所能达到的最好性能，即通过机器学习所能产生的模型精度的理论上限

具体来说，如果目标是最小化分类错误率，则误判损失 $\lambda_{ij}$ 可以写为

$$
\lambda_{ij}\left\{
\begin{aligned}
& 0,\quad if\:i=j;\\
& 1,\quad otherwise;
\end{aligned}
\right.
$$

此时条件风险为

$$
R(c|x)=1-P(c|x)
$$

于是，最小化分类错误率的贝叶最优分类器为

$$
h^*(x)=\argmax\limits_{c\in y}P(c|x)
$$

即对每个样本 $x$ ，选择能使得后验概率 $P(c|x)$ 最大的类别标记

不难看出，使用贝叶斯判定准则来最小化决策风险，首先要获得后验概率；但是在现实中难以直接获得后验概率，机器学习实现的是基于有限的训练样本尽可能准确地估计出后验概率。

主要的两种策略：
- 判别式模型（discriminative models）
    - 给定 $x$ ，通过直接建模 $P(c|x)$ ，来预测 $c$
    - 决策树、BP神经网络、支持向量机
- 生成式模型（generative models）
    - 先对联合概率分布 $P(x,c)$ 建模，由此得到 $P(c|x)$
    - 生成式模型考虑

$$
P(c|x)=\frac{P(x,c)}{P(x)}
$$

<img src="/assets/images/机器学习Bayes.assets/image5.png" alt="image5" style="zoom:80%;">

# 极大似然估计

估计类条件概率的常用策略：先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布参数估计

记关于类别c的类条件概率为 $P(x|c)$
- 假设 $P(x|c)$ 具有服从某个概率分布，分布模型被参数 $\theta_c$ 唯一确定，我们的任务就是利用训练集D估计参数 $\theta_c$

概率模型的训练过程就是参数估计过程，统计学界的两个学派提供了不同的方案：
- 频率主义学派（frequentist）认为参数虽然位置，但存在客观值，因此可以通过优化似然函数等准则来确定参数值
- 贝叶斯学派（bayesian）认为参数是未观察到的随机变量，其本身也可以有分布，因此可假定参数服从一个先验分布，然后基于观测到的数据计算参数的后验分布

令 $D_c$ 表示训练集中第 $c$ 类样本的组合的集合，假设这些样本是独立的，则参数 $\theta_c$ 对于数据集 $D_c$ 的似然是

$$
P(D_c|\theta_c)=\prod\limits_{x\in D_c}P(x|\theta_c)
$$

- 对 $\theta_c$ 进行极大似然估计，寻找能够最大化似然 $P(D_c|\theta_c)$ 的参数值 $\hat{\theta_c}$

为了减少连乘操作造成的下溢，使用对数似然（log-likelihood）

$$
LL(\theta_c)=\log P(D_c|\theta_c)=\sum\limits_{x\in D_c}
\log P(x|\theta_c)
$$

此时参数 $\theta_c$ 的极大似然估计 $\hat{\theta_c}$ 为
$\hat{\theta_c}=\argmax\limits_{\theta_c}LL(\theta_c)$

这种参数化的方法使得类条件概率估计变得相对见到那，但是估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在的真实数据分布

# 朴素贝叶斯分类器

估计后验概率 $P(c|x)$ 主要困难：类条件概率 $P(x|c)$ 是所有属性上的联合概率，难以从有限的训练样本估计获得。

朴素贝叶斯分类器（native bayes classification）采用“属性条件独立性假设”（attribute conditional independence assumption）：每个属性独立地对分类结果发生影响

$$
P(c|x)=\frac{P(c)P(x|c)}{P(x)}
=\frac{P(c)}{P(x)}\prod\limits_{i=1}^d P(x_i|c)
$$

对所有类别来说 $P(x)$ 相同，所以贝叶斯判定准则为

$$
h_{nb}(x)=\argmax\limits_{c\in y}P(c)\prod\limits_{i=1}^d P(x_i|c)
$$

令 $D_c$ 表示训练集 $D$ 中

# 半朴素贝叶斯分类器

<img src="/assets/images/机器学习Bayes.assets/image1.png" alt="image1" style="zoom:50%;">

## SPODE

## TAN

# 贝叶斯网

<img src="/assets/images/机器学习Bayes.assets/image2.png" alt="image2" style="zoom:50%;">

<img src="/assets/images/机器学习Bayes.assets/image3.png" alt="image3" style="zoom:50%;">

<img src="/assets/images/机器学习Bayes.assets/image4.png" alt="image4" style="zoom:50%;">

# EM算法


# 数学基础

- [独立同分布 independent and identically distributed](https://zhuanlan.zhihu.com/p/52530189)
- [如何理解先验概率与后验概率](https://zhuanlan.zhihu.com/p/26464206)
- [机器学习项目中不可忽视的一个密辛 - 大数定理、中心极限定理](https://www.cnblogs.com/LittleHann/p/9569708.html)
- [<基础系列>1：先验概率 & 后验概率](https://zhuanlan.zhihu.com/p/38567891)
- [一文搞懂极大似然估计](https://zhuanlan.zhihu.com/p/26614750)